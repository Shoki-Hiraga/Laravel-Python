import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'other_settingfile'))

import requests
from bs4 import BeautifulSoup
import time
from other_settingfile.rc42_mail import send_notification
from other_settingfile.rc42_mail import send_notification, send_no_match_notification

# === ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ===
BASE_URL = "https://www.goobike.com/maker-honda/car-cb750/index{}.html"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}
PAGINATION_MIN = 1
PAGINATION_MAX = 50

# CSSã‚»ãƒ¬ã‚¯ã‚¿
NO_RESULT_SELECTOR = "tr:nth-of-type(2) [align] span"
DETAIL_LINK_SELECTOR = "span a.detail_kakaku_link"
DETAIL_DATA_SELECTORS = [
    "dt + dd > span", 
    "td[width='15%']:nth-of-type(2)",
    "td[width='15%']:nth-of-type(4)",
]

# åˆ¤å®šç”¨ãƒ†ã‚­ã‚¹ãƒˆ
NO_RESULT_TEXT = "ã”å¸Œæœ›ã®æ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒã‚¤ã‚¯ã¯ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚"

TARGET_SERIAL = "166"

SLEEP_TIME = 2

# === é–¢æ•° ===
def get_page_content(url):
    response = requests.get(url, headers=HEADERS)
    response.encoding = "EUC-JP"
    if response.status_code == 200:
        return BeautifulSoup(response.text, "html.parser")
    return None

def is_no_result_page(soup):
    target = soup.select_one(NO_RESULT_SELECTOR)
    return target and NO_RESULT_TEXT in target.text

def scrape_detail_page(detail_url):
    soup = get_page_content(detail_url)
    if soup:
        print("ğŸ“Œ å–å¾—ãƒ‡ãƒ¼ã‚¿:")
        match_found = False  # â† ä¸€è‡´åˆ¤å®šãƒ•ãƒ©ã‚°

        for selector in DETAIL_DATA_SELECTORS:
            elements = soup.select(selector)
            if elements:
                for el in elements:
                    text = el.text.strip()
                    print(f"ãƒ»{text}")
                    if f"è»Šå°ç•ªå·ä¸‹3æ¡ï¼š{TARGET_SERIAL}" in text:
                        match_found = True
            else:
                print(f"ãƒ»[æœªå–å¾—] ã‚»ãƒ¬ã‚¯ã‚¿: {selector}")
        
        if match_found:
            send_notification(detail_url)

        print("-" * 40)


def main():
    match_found = False  # â† ã“ã“ã§å…¨ä½“ã®ä¸€è‡´ãƒ•ãƒ©ã‚°ã‚’å®šç¾©

    for page in range(PAGINATION_MIN, PAGINATION_MAX + 1):
        print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ {page} ã‚’å‡¦ç†ä¸­...")
        url = BASE_URL.format(page)
        soup = get_page_content(url)
        if not soup:
            print("âš ï¸ ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            break

        if is_no_result_page(soup):
            print("âœ… ãƒã‚¤ã‚¯ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        detail_links = soup.select(DETAIL_LINK_SELECTOR)
        for link in detail_links:
            detail_url = link.get("href")
            if detail_url:
                full_url = "https://www.goobike.com" + detail_url
                print(f"ğŸ” è©³ç´°ãƒšãƒ¼ã‚¸: {full_url}")
                # scrape_detail_page ã‹ã‚‰ä¸€è‡´ãƒ•ãƒ©ã‚°ã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«ã™ã‚‹
                if scrape_detail_page(full_url):
                    match_found = True
                time.sleep(SLEEP_TIME)

        time.sleep(SLEEP_TIME)

    # ä¸€è‡´ã—ãªã‹ã£ãŸå ´åˆã«é€šçŸ¥ã‚’é€ã‚‹
    if not match_found:
        send_no_match_notification()

if __name__ == "__main__":
    main()

