import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from setting_file.header import *
from google.oauth2 import service_account
from googleapiclient.discovery import build
from datetime import datetime, timedelta
from setting_file.Search_Console_set.qshaoh_noindex_url import URLS as Individual_urls

# ================ è¨­å®š ================
SERVICE_ACCOUNT_FILE = api_json.qsha_oh
site_url = 'https://www.qsha-oh.com/'
URLS = Individual_urls  # URLãƒªã‚¹ãƒˆ
file_directory = file_path.file_directory
file_name = "Index_Status_Report_JP.csv"
output_file = os.path.join(file_directory, file_name)
# =====================================

# Search Console APIï¼ˆURL Inspection APIï¼‰
credentials = service_account.Credentials.from_service_account_file(
    SERVICE_ACCOUNT_FILE,
    scopes=['https://www.googleapis.com/auth/webmasters.readonly']
)
inspection_service = build('searchconsole', 'v1', credentials=credentials)

# CSV ãƒ˜ãƒƒãƒ€ãƒ¼
header_row = [
    'URL',
    'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹',
    'æœªç™»éŒ²ã®ç†ç”±',
    'canonical URL',
    'robots.txt çŠ¶æ…‹',
    'Fetch çŠ¶æ…‹',
    'Serving çŠ¶æ…‹'
]


# ================================
# ğŸ”¥ Search Console URL Inspection API
#  canonical / robots / fetch / serving ã‚’ CSV å‡ºåŠ›
# ================================

def translate_index_status(result):
    """
    Search Console UI ã¨å®Œå…¨ä¸€è‡´ã•ã›ãŸæ—¥æœ¬èªç†ç”±åˆ¤å®š
    """
    status = result.get("coverageState", "")
    robots_state = result.get("robotsTxtState", "")
    fetch_state = result.get("pageFetchState", "")
    indexing_state = result.get("indexingState", "")
    verdict = result.get("verdict", "")
    canonical = result.get("canonicalUrl", "")
    ref_canonical = result.get("refCanonical", "")
    serving = result.get("servingStatus", "")

    # âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç™»éŒ²æ¸ˆã¿
    if serving == "SERVING":
        return "âœ… ç™»éŒ²æ¸ˆã¿", "-"

    # âŒ æœªç™»éŒ²ç†ç”±ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆSearch Console UI ã¨å¯¾å¿œï¼‰

    if status == "Excluded by â€˜noindexâ€™ tag" or "noindex" in status.lower():
        return "âŒ æœªç™»éŒ²", "noindex ã‚¿ã‚°ã«ã‚ˆã£ã¦é™¤å¤–ã•ã‚Œã¾ã—ãŸ"

    if robots_state == "BLOCKED_BY_ROBOTS_TXT":
        return "âŒ æœªç™»éŒ²", "robots.txt ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ"

    if verdict == "REDIRECTED":
        return "âŒ æœªç™»éŒ²", "ãƒšãƒ¼ã‚¸ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆãŒã‚ã‚Šã¾ã™"

    if fetch_state == "NOT_FOUND":
        return "âŒ æœªç™»éŒ²", "è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆ404ï¼‰"

    if indexing_state == "PAGE_INDEXING_ISSUE":
        return "âŒ æœªç™»éŒ²", "ã‚¯ãƒ­ãƒ¼ãƒ«æ¸ˆã¿ - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœªç™»éŒ²"

    if canonical and canonical != ref_canonical:
        return "âŒ æœªç™»éŒ²", "ä»£æ›¿ãƒšãƒ¼ã‚¸ï¼ˆé©åˆ‡ãª canonical ã‚¿ã‚°ã‚ã‚Šï¼‰"

    if verdict == "DUPLICATE" or (ref_canonical and canonical != ref_canonical):
        return "âŒ æœªç™»éŒ²", "é‡è¤‡ã—ã¦ã„ã¾ã™ï¼ˆGoogle ã«ã‚ˆã‚Šåˆ¥ãƒšãƒ¼ã‚¸ãŒæ­£è¦ãƒšãƒ¼ã‚¸ã¨ã—ã¦é¸æŠã•ã‚Œã¾ã—ãŸï¼‰"

    return "âŒ æœªç™»éŒ²", status


def inspect_url(url):
    """URL Inspection API ã‚’å®Ÿè¡Œã— CSV ã«å¿…è¦ãªæƒ…å ±ã‚’è¿”ã™"""
    request = {
        "inspectionUrl": url,
        "siteUrl": site_url,
    }

    try:
        response = inspection_service.urlInspection().index().inspect(body=request).execute()
        result = response.get("inspectionResult", {})

        index_data = result.get("indexStatusResult", {})

        index_status, reason = translate_index_status(index_data)

        canonical_url = index_data.get("canonicalUrl", "")
        robots_state = index_data.get("robotsTxtState", "")
        fetch_state = index_data.get("pageFetchState", "")
        serving = index_data.get("servingStatus", "")

        return [url, index_status, reason, canonical_url, robots_state, fetch_state, serving]

    except Exception as e:
        return [url, "ã‚¨ãƒ©ãƒ¼", str(e), "", "", "", ""]

def main():
    indexed_count = 0
    not_indexed_count = 0
    reason_count = {}

    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(header_row)

        for url in URLS:
            delay = random.uniform(1.0, 2.5)
            time.sleep(delay)

            row = inspect_url(url)
            csv_writer.writerow(row)

            # ãƒ­ã‚°ï¼ˆæ—¥æœ¬èªï¼‰
            print(f"URL: {row[0]}")
            print(f" â–¶ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çŠ¶æ…‹: {row[1]}")
            print(f" â–¶ ç†ç”±: {row[2]}")
            print("-------------------------------------------")

            # é›†è¨ˆå‡¦ç†
            if row[1] == "âœ… ç™»éŒ²æ¸ˆã¿":
                indexed_count += 1
            else:
                not_indexed_count += 1
                reason_count[row[2]] = reason_count.get(row[2], 0) + 1

    print("\n============================")
    print("ğŸ“Š ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµæœã¾ã¨ã‚")
    print("============================")
    print(f"âœ… ç™»éŒ²æ¸ˆã¿ãƒšãƒ¼ã‚¸æ•°: {indexed_count}")
    print(f"âŒ æœªç™»éŒ²ãƒšãƒ¼ã‚¸æ•°: {not_indexed_count}")
    print("\nğŸ“Œ æœªç™»éŒ²ç†ç”±å†…è¨³:")

    for reason, count in reason_count.items():
        print(f"ãƒ»{reason}: {count} ä»¶")

    print(f"\nğŸ“ CSVå‡ºåŠ›ã—ã¾ã—ãŸ â†’ {output_file}")


if __name__ == "__main__":
    main()
